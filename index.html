<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ML Team 89</title>
</head>
<body>
    <h1>ML TEAM 89 - Project Proposal and Midterm Checkpoint</h1>

    <h2>Introduction/Background</h2>
    <p>
        Stars, galaxies, and quasars need to be classified to analyze the structures of celestial formations. While telescope observations may be enough, datasets featuring different celestial objects can help classify them autonomously. We will classify stars, galaxies, and quasars in this project.
    </p>

    <h3>Literature Review</h3>
    <p>
        There are several methods to classify celestial objects currently using ML. There is a 2023 paper that compares Random Forest, XGBoost, and K-NN for classifying galaxies, quasars, and stars (Zeraatgari et al.). This paper reported high accuracy for XGBoost and ensemble learning that included Random Forest (Zeraatgari et al.). There is research that suggests that the most important groups of features for classification are photometry, colors, and morphology (von Marttens et al., 2023). Finally, deep learning methods have been used on SDSS data that showed how CNNs and photometric images can be used for classification, with a peak accuracy of 90.7% (He et al., 2021). In comparison to XGBoost and Random Forest’s accuracy of 98.93%, deep learning methods may not perform as well but offer a suitable alternative (Zeraatgari et al.).

    </p>

    <h3>Dataset Description</h3>
    <p>
        The “Stellar Classification Dataset” is from Kaggle. The data is compiled from the Sloan Digital Sky Survey Data Release 17. There are 100,000 rows of data with 17 features/columns. The 18th column is the class, which includes the labels for the data. The labels can be GALAXY, STAR, or QSO (which is quasar).

    </p>

    <h3>Dataset</h3>
    <p>
        Name: Stellar Classification Dataset<br>
URL: <a href="https://www.kaggle.com/datasets/fedesoriano/stellar-classification-dataset-sdss17/data">Dataset URL</a>

    </p>

    <h2>Problem Definition</h2>

    <h3>Problem</h3>
    <p>
        Over 200 probes have been sent to explore celestial bodies outside Earth’s orbit. A significant chunk of these have even been sent into interstellar space. These probes were sent to broaden our understanding of the universe, and as data started flowing in, it was important to catalogue these bodies. We are mainly focusing on stars, galaxies, and quasars. The dataset we are using compiles the characteristics used to do this early cataloguing as well as the results obtained from it.

    </p>

    <h3>Motivation</h3>
    <p>
        Stellar objects such as stars, and galaxies, are observable to the naked eye. However, what we see is just a twinkle in the night sky. To truly know what the role of these objects play, it is important to classify them correctly. By leveraging data collected by probes we can use ML and statistical analysis to distinguish between bodies such as stars, galaxies, and quasars and further our understanding of the universe.

    </p>

    <h2>Methods (Project Proposal)</h2>
    <p>
        In order to perform ML analysis, we need to devise a plan for the pre-processing and ML model methods that we will employ to train the model and evaluate performance. For this we break our methods down into data preprocessing steps where we clean our dataset and ensure computations are efficient, and use ML supervised learning models to train models and validate performances through accuracies and hyperparameter tuning. 
</p>

    <h3>Preprocessing</h3>
    <p>
        Firstly, we look at the preprocessing models that we have. One methodology we plan on employing is feature-pruning, for which we will remove any column entries that are insignificant for the training of our model, especially columns that may just be identifiers. We can use SKLearn function transformers for feature engineering, which we will do as applicable based on scientific reasoning for different data columns like the brightness features. The dataset is relatively clean, but we will use simple imputers for any missing data that we find. For large data values which might be computationally expensive, we will use some form of standardizing to ensure that the data is less expensive to work with. Also for model training, we know that the classes themselves have different numbers of entries, so we will handle class balancing as well.

    </p>

    <h3>Machine Learning Models</h3>
    <p>
        Next, we look at the ML models that we will employ. For these, we use SciKitLearn’s libraries, and fine tune them based on our requirements. We will analyse the performance differences for different models, including Logistic Regression, SVMs, Random Forests, Gradient Boosting, and Multilayer Perceptron NNs. 

    </p>

    <h2>Methods (Midterm Checkpoint)</h2>
    <p>
        As mentioned in our proposal, we decided to do multiple data preprocessing methods to ensure our data is ready to be used to train the model of our choosing. For this midterm checkpoint, we have implemented the following preprocessing methods:
    </p>
    <ol>
        <li><b>Feature Pruning:</b> We started off with this to remove any unnecessary identifiers and metadata that doesn’t particularly help with our classification. This ensures that the model only focuses on the features relevant to our stellar data.</li>
        <li><b>Train-Test Split & Standardization:</b> We then split the data into two subsets: 80% for training, and 20% for testing while also separating the data’s features (X) from the target labels (Y). This ensures all the model’s evaluations are unbiased. We also applied StandardScaler to all numerical features present  to ensure consistent feature scaling. This scaler was then applied to our training data.</li>
        <li><b>Class Balancing:</b> After this, we saw that the class categories were quite imbalanced. Specifically, we observed that ‘GALAXY’ accounted for roughly 59% of the dataset, ‘STAR’ roughly 21%, and 'QSO' roughly 20%. To ensure the model would not be biased towards the majority class, we applied smote on the training data. This creates synthetic samples for the two minority classes (‘STAR’ and ‘QSO’) based on their nearest neighbours thus effectively balancing the distribution of classes in the training dataset.</li>
        <li><b>Label Encoding:</b> Finally, we applied label encoding so that each class (‘GALAXY’, ‘STAR’, ‘QSO’) maps to an integer. This is especially important so that our chosen ML models can properly interpret and process the target data.</li>
    </ol>
    <p>
        Once we were done preprocessing the data, we could start implementing supervised learning methods on the dataset:
    </p>
    <ol>
        <li><b>Logistic Regression:</b> This model serves as a baseline linear classifier. What this means is it helps by finding the best line that separates the three classes - ‘GALAXY’, ‘STAR’, ‘QSO’. It does so by calculating probabilities for each class using the sigmoid function (which is inbuilt). We chose this as it is a good starting point and helps us understand how the data can be separated linearly.</li>
        <li><b>Random Forest:</b> Next we trained a Random Forest model which uses decision trees. It works by building many different decision trees, where each tree is trained on a slightly different part of the data, and combining each tree’s predictions to make a final decision. We chose random forest as although logistic regression is a good starting point, it does not capture complex non linear relationships which are ingrained in datasets like ours. Features like brightness, color, redshift, etc all interact to distinguish between stars, galaxies, and quasars and random forest is good at capturing these interactions in its predictions.</li>
    </ol>

    <h2>Methods (Project Final)</h2>
    <p><b>New model: Multilayer Perceptron (MLP)</b></p>
    <p>
        In the midterm checkpoint we highlighted the next steps we planned on taking, however we have decided to update those planned steps. Firstly, we will be using a MLP (Multilayer Perceptron) model to further analyze our dataset using an artificial neural network. This will be done by analyzing the results using common scores that we implemented for our Random Forest and Logistic Regression models. Finally, we will also be doing photometric analysis on our models to see how they classify our data using g-r values into blue, intermediate, and red objects.
    </p>
    <p>
        We chose to add an MLP specifically because both Logistic Regression and Random Forest showed clear limitations when modeling the nonlinear regions of the data, particularly in the mid-redshift range. This is evident in the “Accuracy based on Redshift Range” graph, where Logistic Regression drops to roughly 75% and Random Forest drops to about 88% for objects with redshift between 0.5 and 1.0. In this region, the photometric magnitudes of galaxies and quasars overlap in ways that cannot be captured by linear boundaries or by tree-based threshold splits alone. Since the mid-redshift region introduces curved, continuous, and interacting feature patterns, we incorporated an MLP to model these nonlinear relationships more effectively through its hidden layers and nonlinear activation functions.
    </p>

    <h2>Results and Discussion (Project Proposal)</h2>
    <p>
        Our primary metrics for the results is the accuracy that we get for various models, based on the model types after fine tuning. We will also look at the per-class precision, meaning that per class we’ll see what percent of objects were correctly predicted for that class. Finally, we will see the F1 score to ensure that the majority class isn’t dominating the entire models.
    </p>
    <p>
        Project goals: we aim to find high accuracy, and identify the model that will provide this highest accuracy. We also want to account for sustainability by ensuring that we maintain computational efficiency with our models and data processing. 
    </p>
    <p>
        
We expect to see high accuracies for all models, with the best performance coming from MLPs (since they handle complex features well) and Random Forests (since they work well for correlated features).

    </p>

    <h2>Results and Discussion (Midterm Checkpoint)</h2>
    <p>Finally, after training the two models highlighted above (Logistic Regression and Random Forest), we must discuss the results obtained from them. For each model, we looked at accuracy, precision, recall, f1 score, confusion matrix, and per class metrics to ensure we have an all-rounded, generic insight into the performance of the model:</p>
    <ol>
        <li><b>Accuracy:</b> measures how often the model’s predictions are correct overall. The Logistic Regression model has an accuracy of 94.65% while the Random Forest model has an accuracy of 97.87%.
        </li>
        <li><b>Precision:</b> tells us how many of the predicted objects for a given class were actually correct. The Logistic Regression model has a precision of 94.79% while the Random Forest model has a precision of 97.86%.</li>
        <li><b>Recall:</b> measures how well the model identified all objects belonging to a given class. The Logistic Regression model has a Recall of 94.65% while the Random Forest model has a Recall of 97.86%.</li>
        <li><b>F1-Score:</b> combines precision and recall into a single number, balancing both. The Logistic Regression model has a F1-Score of 94.66% while the Random Forest model has a F1-Score of 97.86%.</li>
        <li><b>Confusion Matrix:</b> shows detailed counts of correct and incorrect predictions for each class.</li>
        <li><b>Per Class Metrics:</b> highlight the model’s strengths and weaknesses individually.</li>
    </ol>
    <p>
        After analyzing the generic performance of both models, we decided to use Random Forest to dive deeper into the role each feature plays for classification. Each value indicates how much each feature helped reduce uncertainty when splitting the decision trees.
    </p>
    <img src="rf-feat-imp.jpg" width="50%" height="50%" style="display:block; margin-left:auto; margin-right:auto">
    <p>
        From the plot above it is clear that ‘redshift’ contributes the most to reducing uncertainty. This aligns with our understanding of the universe as well as redshift gives us the object's position and velocity with respect to earth, which is an important thing to help distinguish if an object is a star, galaxy, or quasar. Other features such as the photometric magnitudes (u, g, r, i, z) also play some role but no feature comes close to the role that redshift plays.
    </p>
    <p>
        Because of this result, we decided to further analyze the effect redshift plays in determining what a stellar object is. To do this, we first divided the data into 3 groups based on redshift values: low (<0.5), mid (>=0.5 and <1), high (>=1) and then compared the results of the models on these 3 groups.
    </p>
    <img src="acc-redshift.jpg" width="50%" height="50%" style="display:block; margin-left:auto; margin-right:auto">
    <p>
        As seen in the plot above, both models classify much more accurately with low and high redshift values with logistic regression having an accuracy of ~94% and random forest having an accuracy of ~98%. However, when it comes to mid redshift values, the classification accuracy for both models drops considerably (75% for logistic regression and 88% for random forest). This leads to the conclusion that objects with redshift values between 0.5 and 1 are harder to classify and when the model falls back to other features (like photometric magnitudes) it finds that there is overlap in these values between stars, galaxies, and quasars.
    </p>
    <p>
        Thus, through the analyses that we have performed, it is clear that redshift plays a crucial role in classifying stellar objects. Also, from the results it is also clear that Random Forest is much better at handling complex relationships as it consistently performed better (in all metrics) over the logistic regression model.
    </p>
    <h3>Next Steps:</h3>
    <p>For the last part of our project we aim to do the following:</p>
    <ol>
        <li><b>Hyperparameter tuning:</b> We will use techniques like Grid or Random Search to fine tune parameters of the Random Forest model such as tree depth, and minimum samples per split to ensure that our model is not overfitting.
        </li>
        <li><b>Feature Engineering:</b> We will derive new features. For example, we saw in the results that apart from redshift, photometric magnitudes also played a decently high role in classification. So features like differences between photometric magnitudes etc would be good to see how they affect classification.</li>
        <li><b>More model comparisons:</b> We plan to use two additional models: KNN and MLP. By using these models we can further evaluate using distance based clustering and neural networks the classification accuracy of these models.</li>
    </ol>

    <h2>Results and Discussion (Project Final)</h2>
    <p>We will expand on our results from the midterm checkpoint.</p><br>
    <p>We used a MLP with two hidden layers of 100 and 50 neurons because this architecture captures the nonlinear relationships present in photometric and redshift features without overfitting. The MLPClassifier uses ReLU activation function, which introduces nonlinearity and allows the neural network to model nonlinear decision boundaries. This was something both Logistic Regression and Random Forest failed to do in the mid-redshift region. The first layer with 100 neurons learns high-level feature interactions (like how redshift interacts with color indices) and the second 50 neuron layer finds more discriminative patterns for classification. This architecture gave us a balance between classification accuracy and training stability for our dataset, as seen in MLP’s strong performance across all classes.</p>
    <p>This MLP achieved the following results.    </p><br>
    <p><b>Overall performance:</b>   </p>
    <ol>
        <li><b>Accuracy:</b>96.91%</li>
        <li><b>Precision:</b> 96.94%. This tells us that MLP predictions are correct 96.94% of the time.            .</li>
        <li><b>Recall:</b> 96.21%. This tells us that MLP identifies all actual instances of the class 96.21% of the time.        </li>
        <li><b>F1-Score:</b>96.92%. This tells us that the MLP is not biased as there is an almost perfect balance between Precision and Recall.        </li>
        <li><b>Confusion Matrix:</b> Shows classification performance across all 3 classes. For galaxies, 11447 instances were correctly classified while 286 were incorrectly classified as quasars and 106 as stars. For quasars, 3617 instances were correctly classified while 286 were incorrectly classified galaxies and 3 as stars. For stars, 4318 were correctly classified while 28 were incorrectly classified as galaxies and 0 as quasars.        </li>
        <li><b>Per Class Metrics/Performance:</b> For Galaxy classification, precision was 98%, recall 97%, f1 97%. Since there were 11839 instances of galaxies in our dataset, this performance for our largest class is quite good. The small difference between precision and recall suggests that some galaxies are being incorrectly classified (and from our confusion matrix we know that some are incorrectly classified as quasars).
            For Quasar classification, precision was 93%, recall 95%, f1 94%. This class seemed to be the most difficult to classify as it had the lowest accuracy.
            For Star classification, precision was 98%, recall 99%, f1 98%. The model almost perfectly classified all instances of stars (from the confusion matrix we know only 28 instances were misclassified).
            </li>
    </ol>
    <p>Given that we have the same common scores for Random Forest and Logistic Regression, we can compare the three models using the graph shown below:
    </p>
    <img src="finalGraph1.png" width="50%" height="50%" style="display:block; margin-left:auto; margin-right:auto">
    <p>The chart shows that the three models all perform very strongly, with Random Forest achieving the highest precision, recall, f1-score, and accuracy overall. MLP comes second, despite our expectation that the nonlinearity would model a decision boundary better than Random Forest. Logistic regression performs the worst given that it is a linear model that cannot model the same complexity as MLP or Random Forest. Therefore, we can say that the linear decision boundary from logistic regression has been improved on with the introduction of MLP.
    </p>
    <p>
        The confusion matrices show similar results for all 3 models, giving us information about the classes themselves. The order of classes is GALAXY, QSO, STAR. So, we see lower proportions of true positives when classifying QSO and STAR. This tells us that the redshift (most important feature) is not enough to classify these types of stellar objects. This is because the data shows that high redshift objects are often QSOs, while low redshift objects are mostly galaxies. The magnitudes help distinguish stars from galaxies but overlap significantly with quasars in the mid-redshift range. So, we now see a limitation in our results, where all three models could not distinguish effectively between QSO and STAR when relying heavily on redshift feature.
    </p>
    <p>
        This can be seen in the chart below. There is high accuracy in classification for high and low redshift ranges for all 3 models. But in the mid-range, where QSO and STAR lie, there is a heavy drop in accuracy.
    </p>
    <img src="finalGraph2.png" width="50%" height="50%" style="display:block; margin-left:auto; margin-right:auto">
    <p>
        To better understand why our models struggled in the mid-redshift range and for astrophysical context, we introduced a photometric color based evaluation using the g-r color index. It is a measurement of a stellar object color based on the difference in brightness between its green (g) and red (r) light. If a stellar object has a low (g-r) index, it will be hotter and appear bluer or whiter. If it has high (g-r) index, it is cooler and appears redder.
    </p>
    <img src="finalGraph3.png" width="50%" height="50%" style="display:block; margin-left:auto; margin-right:auto">
    <p>By analyzing the test results using 3 groups (blue, intermediate, and red objects) we were able to measure how classification accuracy changes for different types of stellar objects. This analysis reveals patterns that are not visible in overall accuracy alone. All models perform strongly on intermediate (Blue-Red) objects, and the MLP shows a clear advantage on red objects (which are likely stars based on the next graph). These results demonstrate that color provides additional classification ability beyond redshift and magnitudes, and highlight how nonlinear models like the MLP benefit from photometric data. This step allowed us to explore where model performance improves or drops based on another feature, giving us a deeper understanding of classification behavior. We can see this using the graph below:
    </p>
    <img src="finalGraph4.png" width="50%" height="50%" style="display:block; margin-left:auto; margin-right:auto">
    <p>
        The above graph explains why these performance differences arise. The histogram of true g-r colors reveals that galaxies cluster around intermediate color values (g-r ranging from 0.0-0.5), stars span a wider range of both blue and near-red colors, and quasars occupy the bluest end of the distribution (g-r < -0.3). However, the distributions overlap significantly between stars and galaxies, and between stars and quasars in the mid-blue region. This overlap directly reflects the patterns seen in the confusion matrices for all three models, since the proportion of overlap is greater for STAR and QSO. This means the misclassifications evident in the confusion matrices for STAR and QSO can be explained by how several features are very similar for STAR and QSO, which share very similar g-r color profiles.
    </p>
    <p>
        Overall, our results show that while all three models perform well on this dataset, Random Forest remains the strongest classifier, with MLP providing competitive performance and clear improvements over Logistic Regression. This is because MLP introduces layers with nonlinearity that help classify more complex relationships. We optimized our models very well, and investigated various features like redshift and photometric features to further investigate classifying between STAR and QSO objects. We were able to demonstrate where each model succeeds using the several graphs above. Through this expanded analysis, we achieved a more complete understanding of the dataset, the behavior of our models, and the astrophysical factors that affect classification performance.
    </p>
    <h2>References</h2>
    <p>
        Zeraatgari, Fatemeh Zahra, et al. “Machine Learning-Based Photometric Classification of Galaxies, Quasars, Emission-Line Galaxies, and Stars.” 
        Monthly Notices of the Royal Astronomical Society, vol. 527, no. 3, 2023, pp. 4677–89. Oxford University Press, 
        <a href="https://doi.org/10.1093/mnras/stad3436" target="_blank">https://doi.org/10.1093/mnras/stad3436</a>.
    </p>
    <p>
        He, Z., B. Qiu, A-Li Luo, J. Shi, X. Kong, and X. Jiang. “Deep Learning Applications Based on SDSS Photometric Data: Detection and Classification of Sources.” 
        Monthly Notices of the Royal Astronomical Society, vol. 508, no. 2, 2021, pp. 2039–52. Oxford University Press, 
        <a href="https://doi.org/10.1093/mnras/stab2243" target="_blank">https://doi.org/10.1093/mnras/stab2243</a>.
    </p>
    <p>
        Omat, D., J. Otey, and A. Al-Mousa. “Stellar Objects Classification Using Supervised Machine Learning Techniques.” 
        2022 International Arab Conference on Information Technology (ACIT), 2022. IEEE, 
        <a href="https://doi.org/10.1109/acit57182.2022.9994215" target="_blank">https://doi.org/10.1109/acit57182.2022.9994215</a>.
    </p>
    <p>
        von Marttens, R., et al. “J-PLUS: Galaxy–Star–Quasar Classification for DR3.” 
        Monthly Notices of the Royal Astronomical Society, vol. 527, no. 2, 2023, pp. 3347–65. Oxford University Press, 
        <a href="https://doi.org/10.1093/mnras/stad3373" target="_blank">https://doi.org/10.1093/mnras/stad3373</a>.
    </p>

    <h2>Gantt Chart</h2>
    <p><a href="https://docs.google.com/spreadsheets/d/17i_9VykRdETPD9Dd1OORQ9lNznGvdez-6PSElU9oeYc/edit?usp=sharing">Gantt Chart Link</a></p>

    <h2>Contributions</h2>
    <table border="1">
        <tr>
            <th><b>Name</b></th>
            <th><b>Final Contributions</b></th>
        </tr>
        <tr>
            <td>Ved Srivathsa</td>
            <td>Feature Importance & Redshift Analysis, Random Forest Results, Report Writing
            </td>
        </tr>
        <tr>
            <td>Vasanth Gogineni</td>
            <td>Logistic Regression Results, MLP Results, Report Writing
            </td>
        </tr>
        <tr>
            <td>Akshay Bhave</td>
            <td>Data Preprocessing, MLP results, Report Writing
            </td>
        </tr>
        <tr>
            <td>Lindsey Ehrlich</td>
            <td>Potential Results and Discussion, Random Forest Results, Report Writing
            </td>
        </tr>
        <tr>
            <td>Arnav Jha</td>
            <td>Model Training, Report Writing, Final Presentation Slides
            </td>
        </tr>
    </table>
    <br>
    <table border="1">
        <tr>
            <th><b>Name</b></th>
            <th><b>Midterm Contributions</b></th>
        </tr>
        <tr>
            <td>Ved Srivathsa</td>
            <td>Feature Importance & Redshift Analysis, Report Writing
            </td>
        </tr>
        <tr>
            <td>Vasanth Gogineni</td>
            <td>Logistic Regression Results, Report Writing
            </td>
        </tr>
        <tr>
            <td>Akshay Bhave</td>
            <td>Data Preprocessing, Report Writing
            </td>
        </tr>
        <tr>
            <td>Lindsey Ehrlich</td>
            <td>Random Forest Results, Report Writing
            </td>
        </tr>
        <tr>
            <td>Arnav Jha</td>
            <td>Model Training, Report Writing
            </td>
        </tr>
    </table>
    <br>
    <table border="1">
        <tr>
            <th><b>Name</b></th>
            <th><b>Project Proposal Contributions</b></th>
        </tr>
        <tr>
            <td>Ved Srivathsa</td>
            <td>Methods, Video Recording, Github Page
            </td>
        </tr>
        <tr>
            <td>Vasanth Gogineni</td>
            <td>Introduction/Background, Video Recording, Github Page
            </td>
        </tr>
        <tr>
            <td>Akshay Bhave</td>
            <td>Problem Definition, Video Recording, Github Page
            </td>
        </tr>
        <tr>
            <td>Lindsey Ehrlich</td>
            <td>Potential Results and Discussion, Github Page
            </td>
        </tr>
        <tr>
            <td>Arnav Jha</td>
            <td>References, Github Page
            </td>
        </tr>
    </table>
</body>
</html>
